{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure for the Data Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand the Evloving World of Data\n",
    "\n",
    "- Learn the key factors that are driving changes in data generation, roles, and technologies.\n",
    "- Compare the differences between on-premises data technologies and cloud data technologies.\n",
    "- Outline how the role of the data professional is changing in organizations.\n",
    "- Identify use cases that involve these changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Abundance\n",
    "- view data on PCs, tablets, and mobile devices\n",
    "- Data forms include text, stream, audio, video, and metadata. Data can be structured, unstructured, or aggregated. \n",
    "- must maintain data systems that are accurate, highly secure, and constantly available. The systems must comply with applicable regulations \n",
    "-  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud v On Premises Servers\n",
    "### OnPremise  \n",
    "- requires physical servers\n",
    "- individual CALs and OS\n",
    "- self-maintained\n",
    "- scale through adding more servers\n",
    "- higher uptime (Availability) increases cost and complexity\n",
    "- TCO\n",
    "  - Hardware\n",
    "  - Software licensing\n",
    "  - Labor (installation, upgrades, maintenance)\n",
    "  - Datacenter overhead (power, telecommunications, building, heating and cooling)\n",
    "### Cloud\n",
    "- no physical devices\n",
    "- no licensing or CALs\n",
    "- maintenance included\n",
    "- scale automatically\n",
    "- higher uptime (Availability) increases cost and complexity\n",
    "- multilingual support built in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Responsibilities\n",
    "\n",
    "### Data Engineer\n",
    "- Traditionally does an ETL DW  \n",
    "- Now focusing on ELT and DataLakes  \n",
    "- now provision server needs instead if implementing new servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases for Cloud\n",
    "- Web\n",
    "- healthcare\n",
    "- IoT solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Questions\n",
    "- Which data processing framework will a data engineer use to ingest data onto cloud data platforms in Azure?\n",
    "  - ELT\n",
    "- The schema of what data type can be defined at query time?\n",
    "  - unstructured data\n",
    "- Duplicating customer content for redundancy and meeting service-level agreements (SLAs) in Azure meets which cloud technical requirement?\n",
    "  - High Availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survey the Services on the Azure Data Platform\n",
    "\n",
    "- Contrast structured data with nonstructured data.\n",
    "- Explore common Azure data platform technologies and identify when to use them.\n",
    "- List additional technologies that support the common data platform technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data Types\n",
    "### Structured data\n",
    "In relational database systems data structure is defined at design time. Data structure is designed in the form of tables.\n",
    "### Nonstructured data\n",
    "binary, audio, and image files  \n",
    "Nonstructured data is stored in nonrelational systems, commonly called unstructured or NoSQL systems. \n",
    "\n",
    "### 4 types of NoSQL databases:\n",
    "\n",
    "- Key-value store: Stores key-value pairs of data in a table structure.\n",
    "- Document database: Stores documents that are tagged with metadata to aid document searches\n",
    "- Graph database: Finds relationships between data points by using a structure that's composed of vertices and edges.\n",
    "- Column database: Stores data based on columns rather than rows. Columns can be defined at the query's runtime, allowing flexibility in the data that's returned performantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand Data Storage in Azure Storage\n",
    "### 4 configuration options:\n",
    "\n",
    "Azure Blob: A scalable object store for text and binary data\n",
    "Azure Files: Managed file shares for cloud or on-premises deployments\n",
    "Azure Queue: A messaging store for reliable messaging between application components\n",
    "Azure Table: A NoSQL store for no-schema storage of structured data\n",
    "\n",
    "### When to use Blob storage\n",
    "- Provision a data store that will store but not query data\n",
    "- Cheapest option\n",
    "- Blob storage works well withunstructured data\n",
    "\n",
    "### Key Features\n",
    "- scalable and secure\n",
    "- durable\n",
    "- highly available\n",
    "- handles hardware maintenance, updates, and critical issues\n",
    "- provides REST APIs and SDKs \n",
    "- supports .NET, Node.js, Java, Python, PHP, Ruby, and Go\n",
    "- supports scripting in Azure PowerShell and the Azure CLI\n",
    "\n",
    "### Data Ingestion\n",
    "\n",
    "- Azure Data Factory\n",
    "- Azure Storage Explorer\n",
    "- Apache Sqoop\n",
    "- the AzCopy tool\n",
    "- PowerShell\n",
    "- Visual Studio\n",
    "- File Upload feature to import file sizes above 2 GB, use PowerShell or Visual Studio\n",
    "- AzCopy supports a maximum file size of 1 TB and automatically splits data files that exceed 200 GB\n",
    "\n",
    "### Data Security\n",
    "- Azure Storage encrypts all data that's written to it\n",
    "- Azure Storage securesdata by using keys or shared access signatures\n",
    "- Azure Resource Manager provides a permissions model that uses role-based access control (RBAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Azure Data Lake Storage\n",
    "\n",
    "- Hadoop-compatible data repository\n",
    "- Store any size or type of data\n",
    "- Available as Generation 1 (Gen1) \n",
    "\n",
    "### Gen2\n",
    "- Azure Blob storage\n",
    "- hierarchical file system\n",
    "- performance tuning\n",
    "- access data through \n",
    "  - Blob API\n",
    "  - Data Lake file API\n",
    "- Storage layer for Databricks, Hadoop, HDInsight\n",
    "\n",
    "\n",
    "### When Data Lake\n",
    "Data Lake Storage is designed to store massive amounts of data for big-data analytics.\n",
    "\n",
    "### DataLake Features\n",
    "\n",
    "- Unlimited scalability\n",
    "- Hadoop compatibility\n",
    "- Security support for both access control lists (ACLs)\n",
    "- POSIX compliance\n",
    "- An optimized Azure Blob File System (ABFS) driver that's designed for big-data analytics\n",
    "- Zone-redundant storage\n",
    "- Geo-redundant storage\n",
    "\n",
    "### DataLake Queries\n",
    "- Gen1, data engineers query data by using the U-SQL language  \n",
    "- Gen 2, use the Azure Blob Storage API or the Azure Data Lake System (ADLS) API.\n",
    "\n",
    "### DataLake Security\n",
    "- Azure Active Directory ACLs - Active Directory Security Groups\n",
    "- Role-based access control (RBAC) in Gen1 and Gen2\n",
    "  - Built-in security groups include ReadOnlyUsers, WriteAccessUsers, and FullAccessUsers\n",
    "- Data Lake Storage automatically encrypts data at rest,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosmos DB\n",
    "- globally distributed, multimodel database  \n",
    "- Deploy with API models\n",
    "  - SQL API\n",
    "  - MongoDB API (unstructured)\n",
    "  - Cassandra API (Wide columns)\n",
    "  - Gremlin API (Graph database)\n",
    "  - Table API\n",
    "\n",
    "### When Cosmos DB\n",
    "- Need a NoSQL database of the supported API model\n",
    "  - at planet scale\n",
    "  - with low latency performance\n",
    "- Currently, Azure Cosmos DB supports five-nines uptime (99.999 percent). It can support response times below 10 ms when it's provisioned correctly.\n",
    "- Consistency levels include strong, bounded staleness, session, consistent prefix, and eventual.\n",
    "\n",
    "### Ingest Data into Cosmos \n",
    "with Azure DataFactory\n",
    "\n",
    "### Queries\n",
    "- stored procedures\n",
    "- triggers\n",
    "- user-defined functions (UDFs)\n",
    "- JavaScript query API\n",
    "- other APIs\n",
    "  - Data Explorer component uses the graph visualization panel\n",
    "\n",
    "### Security\n",
    "- data encryption\n",
    "- IP firewall configurations\n",
    "- access from virtual networks\n",
    "-  Data is encrypted automatically\n",
    "-  User authentication is based on tokens\n",
    "-  Azure AD provides role-based security\n",
    "-  Security compliance certifications HIPAA, FedRAMP, SOX, and HITRUST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand Azure SQL Database\n",
    "- managed relational database service\n",
    "- supports structures such as relational data and unstructured formats\n",
    "  - spatial\n",
    "  - XML data\n",
    "- provides online transaction processing (OLTP) that can scale on demand\n",
    "- comprehensive security and availability\n",
    "\n",
    "### When Azure SQL Database\n",
    "- need to scale up and scale down OLTP systems on demand\n",
    "- take advantage of Azure security and availability features\n",
    "- flexible - provision in minutes\n",
    "- backed by Azure SLA\n",
    "\n",
    "### Key Features\n",
    "- predictable performance\n",
    "- minimal admin\n",
    "- dynamic scalability\n",
    "- optimization\n",
    "- high availability\n",
    "- advanced security\n",
    "\n",
    "### Ingesting and Processing Data\n",
    "- application integration from developer SDKs\n",
    "  - .NET\n",
    "  - Node.js\n",
    "  - Python\n",
    "  - Java\n",
    "- Transact-SQL (T-SQL) techniques \n",
    "- Azure Data Factory\n",
    "\n",
    "### Queries\n",
    "With T-SQL\n",
    "\n",
    "### Security\n",
    "- Advanced Threat Protection\n",
    "- SQL Database auditing\n",
    "- Data encryption\n",
    "- Azure Active Directory authentication\n",
    "- Multi-factor authentication\n",
    "- Compliance certification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Synapse Analytics\n",
    "a cloud-based data platform that brings together enterprise data warehousing and Big Data analytics  \n",
    "\n",
    "### When Azure Synapse\n",
    "- reduced processing times\n",
    "- release reports faster\n",
    "\n",
    "### Key Features\n",
    "- SQL pools use massively parallel processing (MPP) across petabytes\n",
    "- scale compute separately\n",
    "- Data Movement Service (DMS) to balance compute (can pause and resume - pay as you use)\n",
    "- Distributed tables\n",
    "  - hash\n",
    "  - round-robin\n",
    "  - replicated\n",
    "  - these tables tune performance\n",
    "\n",
    "### Ingesting and Processsing Data\n",
    "- ELT process\n",
    "  - bcp\n",
    "  - SQLBulkCopy API\n",
    "- Polybase\n",
    "  - lowers complexity\n",
    "  - applies stored procedures\n",
    "  - labels\n",
    "  - views\n",
    "\n",
    "### Queries\n",
    "T-SQL\n",
    "\n",
    "### Security\n",
    "- SQL Server and Azure AD authentication\n",
    "- row and column level security"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Stream Analytics\n",
    "Applications, sensors, monitoring devices, and gateways broadcast continuous event data known as data streams. Streaming data is high volume and has a lighter payload than nonstreaming systems.\n",
    "\n",
    "### When Stream Analytics\n",
    "- respond to data events in real time\n",
    "- analyze large batches in continuous time-bound stream\n",
    "- events into an Event hun or IoT hub\n",
    "  - then streamed to Stream Analytics\n",
    "- can't wait for batch sytems\n",
    "  - autonomous driving\n",
    "\n",
    "### Ingest Data\n",
    "- Azure Event Hubs\n",
    "  - big data streaming\n",
    "  - large customer request volume\n",
    "  - push to databricks, stream analytics, datalake, hdinsight \n",
    "- Azure Iot Hub\n",
    "  - bisirectional - data in and commands/policy back to IoT devices\n",
    "- Azure Blob Storage\n",
    "  - batch processing\n",
    "\n",
    "### Process Data\n",
    "- Inputs \n",
    "  - Event Hubs\n",
    "  - IoT Hubs\n",
    "  - Azure Storage\n",
    "- Outputs\n",
    "  - Azure Blob\n",
    "  - Azure SQL Database\n",
    "  - Azure Data Lake Storage\n",
    "  - Azure Cosmos DB\n",
    "- Analysis after storing\n",
    "  - Run batch analytics in Azure HDInsight\n",
    "  - Or send the output to a service (e.g. Event Hubs) for consumption\n",
    "  - Or use the Power BI streaming API to send the output to Power BI for real-time visualization.\n",
    "\n",
    "## Queries\n",
    "- Stream Analytics language\n",
    "  - SQL plus complex temporal queries\n",
    "\n",
    "### Security\n",
    "- transport layer\n",
    "- data discarded after use\n",
    "- use storage method to store encrypted data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HD Insight\n",
    "Azure HDInsight provides technologies to help you ingest, process, and analyze big data. It supports batch processing, data warehousing, IoT, and data science.\n",
    "\n",
    "### Key Features\n",
    "- low-cost cloud solution\n",
    "- includes \n",
    "  - Apache Hadoop\n",
    "    - Hadoop includes \n",
    "      - Apache Hive, HBase, Spark, and Kafka. \n",
    "      - Hadoop stores data in a file system (HDFS)\n",
    "  - Spark\n",
    "    - Spark stores data in memory\n",
    "    - makes Spark about 100 times faster\n",
    "  - HBase Kafka, , and Interactive Query\n",
    "    - HBase is a NoSQL database built on Hadoop\n",
    "  - Storm\n",
    "    - distributed real-time streaming analytics solution.\n",
    "  - Kafka\n",
    "    - open-source platform that's used to compose data pipelines\n",
    "    - offers message queue functionality,\n",
    "    - allows publish or subscribe to real-time data streams.\n",
    "\n",
    "### Ingesting Data\n",
    "use Hive to run ETL operations or orchestrate Hive queries in ADF\n",
    "\n",
    "### Data Processing\n",
    "- Hadoop\n",
    "  - Python\n",
    "  - Java\n",
    "  - Mapper consumes and analyzes input data\n",
    "  - Reducer can analyse can run summary operations to reduce result set\n",
    "- Spark\n",
    "  - Spark Streaming\n",
    "  - Anaconda Python libraries\n",
    "  - GrpahX\n",
    "- Storm\n",
    "  - Java\n",
    "  - C#\n",
    "  - Python\n",
    "\n",
    "### Queries\n",
    "- Hadoop\n",
    "  - Pig\n",
    "  - HiveQL\n",
    "- Spark\n",
    "  - Spark SQL\n",
    "\n",
    "### Security\n",
    "- encryption\n",
    "- SSH\n",
    "- shared access signatures\n",
    "- Azure aD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Data Services\n",
    "\n",
    "### Databricks\n",
    "- serveless platform\n",
    "- one-click setup\n",
    "- streamlined workflows\n",
    "- interactive workspace\n",
    "- Spark based\n",
    "- REST APIs to program Spark clusters\n",
    "- Notebooks\n",
    "  - R, Python, Scala, SQL\n",
    "\n",
    "### Data Factory\n",
    "Use Data Factory to organize raw data into meaningful data stores and data lakes so your organization can make better business decisions  \n",
    "- cloud integration service\n",
    "- orchestrates movement between data stores\n",
    "- create and schedule pipelines\n",
    "- Compute with\n",
    "  - HD Insight\n",
    "  - Hadoop\n",
    "  - Spark\n",
    "- Publish to \n",
    "  - Snapyse Analytics\n",
    "  - data stores\n",
    "\n",
    "### Azure Purview\n",
    "- a unified data governance service that helps you manage and govern your on-premises, multicloud, and software-as-a-service (SaaS) data\n",
    "- create a holistic, up-to-date map of your data landscape with automated data discovery, sensitive data classification, and end-to-end data lineage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Questions\n",
    "- Which data platform technology is a globally distributed, multimodel database that can perform queries in less than a second?\n",
    "  - Azure Cosmos DB\n",
    "- Which data store is the least expensive choice when you want to store data but don't need to query it?\n",
    "  - Azure Storage\n",
    "- Which Azure service is the best choice to store documentation about a data source?\n",
    "  - Azure Purview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineer Tasks in Cloud-Hosted Architecture\n",
    "- List the work roles involved in modern data projects\n",
    "- Outline data engineering practices\n",
    "- Explore the high-level process for designing a data engineering project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Roles\n",
    "### Data Engineer\n",
    "- provison and set up platform technologies\n",
    "- manage data flows\n",
    "  - databases (relational or non)\n",
    "  - data streams\n",
    "  - file stores\n",
    "- integrate data services\n",
    "- ingest, egress, transform data from multiple sources\n",
    "- collaborate with stakeholders\n",
    "- manage security\n",
    "- data wrangling\n",
    "  - ingest, egress, transform, validata, and clean data\n",
    "\n",
    "### Data Scientist\n",
    "- advanced analytics to extract value\n",
    "- descriptive, predictive, prescriptive analytics\n",
    "- forecast models\n",
    "- deep learning\n",
    "- build models\n",
    "\n",
    "### AI Engineer\n",
    "- work with cognitive services, cognitive search, bot framework\n",
    "- computer vison, text analysis\n",
    "- use APIS over models\n",
    "- add the intelligent capabilities of vision, voice, language, and knowledge to applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering Processes\n",
    "- Design and develop data storage and data processing solutions for the enterprise\n",
    "- Set up and deploy cloud-based data services such as blob services, databases, and analytics\n",
    "- Secure the platform and the stored data\n",
    "- Ensure business continuity in uncommon conditions by using techniques for high availability and disaster recovery\n",
    "- Monitor to ensure that the systems run properly and are cost-effective.\n",
    "\n",
    "## Move Data Around\n",
    "- Extract\n",
    "  - define data source (resource group, subscription, identity)\n",
    "  - define the data (query, files, blob storage)\n",
    "- Transform\n",
    "  - define transformation (splitting, combining, deriving, adding, removing, pivoting, aggreagating, merging)\n",
    "- Load\n",
    "  - define the destination - (JSON, file, blob, API)\n",
    "    - Node.js, .NET, Python, and Java. \n",
    "    - Extensible Markup Language (XML)common in the past\n",
    "    - most now use JSON because of its flexibility as a semistructured data type\n",
    "  - start the job (test and run)\n",
    "  - monitor the job (ensure it completes or troubleshoot and rerun)\n",
    "\n",
    "### ETL Tools\n",
    "- Azure Data Factory\n",
    "- Azure Purview for data source info and data dictionaries\n",
    "\n",
    "### Evolution from ETL\n",
    "- Move to ELT\n",
    "- Stores data in original format\n",
    "  - reduces load time\n",
    "\n",
    "### Holistic Data Engineering\n",
    "- Phases of Data Projects\n",
    "  - Source: data systems to extract\n",
    "  - Ingest: identify tech and methods\n",
    "  - Prepare: identify tech and methods\n",
    "  - Analyze: identify tech and method\n",
    "  - Consume: identify tech and method\n",
    "- Not necessarily a liner flow between phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Data Engineers Tasks\n",
    "### Five Phases\n",
    "- source\n",
    "- ingest\n",
    "- prepare\n",
    "- analyze\n",
    "- consume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Questions\n",
    "- Which role works with Azure Cognitive Services, Cognitive Search, and the Bot Framework?\n",
    "  - AI Engineer\n",
    "- Which Azure data platform is commonly used to process data in an ELT framework?\n",
    "  - Azure Data Factory"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
